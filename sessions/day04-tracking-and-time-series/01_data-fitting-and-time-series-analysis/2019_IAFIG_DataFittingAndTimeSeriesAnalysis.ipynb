{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Fitting and Time Series Practical.\n",
    "\n",
    "by Dominic Waithe (2019)  \n",
    "\n",
    "Exercise:   \n",
    "To demonstrate and utilise different tools for data processing and fitting. We show regression, time series analysis and Gaussian fitting.\n",
    "\n",
    "\n",
    "Instructions:  \n",
    "Work through the notebook cell-by-cell. Where you see TODO, this means you need to do something. Optional TODO means you should do it if you are finding things too easy and have enough time.\n",
    "Many of the exercises below utilise the Skimage Python library, if in doubt, google the function names, to find out additional description.\n",
    "Some of the cells also involve plotting. If you are having trouble understanding the plotting with Matplotlib, then I recommend the following text: https://jakevdp.github.io/PythonDataScienceHandbook/04.00-introduction-to-matplotlib.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Library imports for this notebook. Please remember to run before any other cells.\n",
    "%matplotlib inline\n",
    "from scipy import ndimage as ndi\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import data, img_as_float,color, img_as_ubyte\n",
    "from skimage import color, io\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from matplotlib import cm\n",
    "import numpy as np\n",
    "import pickle\n",
    "import tifffile\n",
    "from lmfit import minimize, Parameters,report_fit,report_errors, fit_report\n",
    "#pip install git+https://github.com/dwaithe/nanosimpy\n",
    "from nanosimpy import equations_to_fit as eq\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression example\n",
    "Here we show an example of linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = np.array([[18,  510],[20,  590],[22,  560],[23,  510],[23,  460],[25,  490],[27,  560],[28,  510],\n",
    "[29,  460],[32,  410],[37,  420],[41,  460],[46,  450],[49,  380],[53,  460],[55,  420],[63,  350],\n",
    "[65,  420],[66,  300],[67,  410],[68,  300],[70,  390],[71,  320],[72,  370],[73,  280],[74,  420],\n",
    "[75,  460],[77,  360],[79,  310],[82,  360]])\n",
    "\n",
    "\n",
    "n = data.shape[0]\n",
    "x = data[:,0]\n",
    "y = data[:,1]\n",
    "\n",
    "#Using the analytical solution for linear regression which is in the notes:\n",
    "b0 = (np.average(y)*(np.sum(x**2))-np.average(x)*np.sum(x*y))/(np.sum(x**2)-n*np.average(x)**2)\n",
    "print('b0',b0) \n",
    "# equation (2).\n",
    "b1 = ((np.sum(x*y))-n*np.average(x)*np.average(y))/(np.sum(x**2)-n*np.average(x)**2)\n",
    "print('b1',b1)\n",
    "#visualisation\n",
    "xx = np.linspace(np.min(x),np.max(x),2)\n",
    "yy = np.array(b0+ b1 * xx)\n",
    "\n",
    "#plotting.\n",
    "plt.plot(xx,yy,'-',color='pink')\n",
    "plt.scatter(data[:,0], data[:,1],color='k')\n",
    "plt.title(\"XY plot comparing the independent variable and the dependent variable \")\n",
    "plt.xlabel(\"Independent variable (x)\")\n",
    "plt.ylabel(\"Dependent variable (y)\");\n",
    "\n",
    "\n",
    "#TODO:\n",
    "#Using the function available through the below link, implement linear regression on the data above.\n",
    "#https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html\n",
    "#You will have to format you X input a little differently. use x = data[:,np.axis,0]. This is because it is capable of multi-dimensional regression.\n",
    "#You should get the same b0 and b1 values as above.\n",
    "\n",
    "\n",
    "#Visit online the interactive demonstration:\n",
    "#http://userweb.molbiol.ox.ac.uk/dwaithe/WIMM_Advanced_Imaging/linearRegressiond4.html\n",
    "#Can you get the same parameters as the fit. Do you agree with the choice? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation of time series data\n",
    "Remember this from the lecture? This is a visualisation illustrating what happens during a Fluorescence Correlation Spectroscopy experiment.  \n",
    "![FCS intensity](src/pointFCS_ani.gif \"segment\")  \n",
    "Fluorescently tagged proteins/molecules pass through a microscope illumination spot and become fluorescent.\n",
    "The fluorescence is measured over time to generate an intensity time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To extract meaning from this data, we want to analyse the fluctuations and measure how long they are. This will allow us to determine how fast the\n",
    "molecules within the sample are diffusing. To do this we correlate the intensity time trace we measure from the above example.\n",
    "This is a graphical representation for correlation:  \n",
    "![FCS intensity](src/FCS_intensity_correlation.gif \"segment\")   \n",
    "If you want this animation to stop, just double-click this cell.\n",
    "The dark blue trace represents our time series. The light-blue trace is the same trace but is moved along the sequence and compared at every point.\n",
    "Below is the function which performs this correlation on our input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlate_linear(a, b):\n",
    "    \"\"\"Return linear correlation of two vectors using DFT.\"\"\"\n",
    "    size = a.size\n",
    "        \n",
    "    a_mean = a.mean()\n",
    "    b_mean = b.mean()\n",
    "    \n",
    "    # subtract mean and pad with zeros to twice the size\n",
    "    a = np.pad(a-a_mean, a.size//2, mode='constant')\n",
    "    b = np.pad(b-b_mean, b.size//2, mode='constant')\n",
    "    \n",
    "    # forward DFT\n",
    "    a = np.fft.rfft(a)\n",
    "    b = np.fft.rfft(b)\n",
    "    # multiply by complex conjugate\n",
    "    c = a * b.conj()\n",
    "    # reverse DFT\n",
    "    c = np.fft.irfft(c)\n",
    "    # positive delays only\n",
    "    c = c[:size // 2]\n",
    "        \n",
    "    # normalize with the averages of a and b\n",
    "    c /= size * a_mean * b_mean\n",
    "    \n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we need to import some data.\n",
    "file_dict = pickle.load(open('images/parameters.pickle', 'rb'))\n",
    "time_step = 1. #ms. This is the time-step. We need this to calibrate our plotting.\n",
    "data = file_dict['trace'][0]\n",
    "_time = np.arange(data.shape[0])*time_step\n",
    "#We then plot it:\n",
    "plt.plot(_time,data);\n",
    "plt.title(\"Our Intensity Time Trace\")\n",
    "plt.ylabel(\"Intensity\")\n",
    "plt.xlabel(\"Time (ms)\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we autocorrelate our time series by running it through our correlate function.\n",
    "#We now want to do auto-correlation. To do this call correlate_linear() on our data. You will need to pass the variable data for both inputs.\n",
    "corr_data = #insert correlate_linear here.\n",
    "_time_corr = np.arange(corr_data.shape[0])*time_step\n",
    "#We plot FCS on a logarithmic access to get the characteristic curve.\n",
    "plt.semilogx(_time_corr,corr_data)\n",
    "plt.xlabel('Tau (ms)')\n",
    "plt.ylabel('Autocorrelation');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We set our initialisation parameters of our fit. \n",
    "param = Parameters()\n",
    "param.add('offset', value=0.01, min=-0.5, max=1.5, vary=True); #Controls the offset of the fit from the baseline (e.g. 0)\n",
    "#TODO: \n",
    "#Above we have added one parameter to our fit initialisation. We need to add some more.\n",
    "#Please add the following in a similar way to the parameter 'offset' is added above:\n",
    "#'GN0' value=3000 min=-0.0001 max=3000.0 vary=True    #Controls the amplitude (i.e. height) of the curve.\n",
    "#'A1' value=1.000 min=0.0001 max=1.0000 vary=False #If we had more than one diffusing species we could change this.\n",
    "#'txy1' value=0.10 min=0.001 max=2000.0 vary=True  #This is the apparent transit time of the diffusing species through the spot.\n",
    "#'alpha1' value=1 min=0.2 max=2.0 vary=True        #This is our anomalous factor which allows non-ideal diffusion. \n",
    "#If the code runs, then the model parameters are working correctly. The model won't fit well, without changeing some of the parameter values (see TODO below).\n",
    "\n",
    "#The equation for our model can be seen here: https://github.com/dwaithe/nanosimpy/blob/master/nanosimpy/equations_to_fit.py\n",
    "options = {'Dimen':1,'Diff_eq':1,'Triplet_eq':1,'Diff_species':1} #Here we select our specific model. Don't worry about this at this stage.\n",
    "#This is our minimize function, which optimises our model parameters:\n",
    "#eq.residual evaluates the loss between our model and our data.\n",
    "res = minimize(eq.residual, param, args=(_time_corr,corr_data, options))\n",
    "\n",
    "#These are the output residuals.\n",
    "residualVar = res.residual\n",
    "\n",
    "output = fit_report(res.params)\n",
    "print ('residual',res.chisqr)\n",
    "plt.semilogx(_time_corr,corr_data,'go')\n",
    "plt.semilogx(_time_corr,eq.equation_(res.params,_time_corr,options),'r')\n",
    "\n",
    "#TODO:\n",
    "#Once you run it you will see that the initial values of the parameters are not great, and the model (red) does not fit the data (green).\n",
    "#Change the parameter values to see if you can get a good model fit.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This will print our output variables for our model.\n",
    "res.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#You can access the learnt parameters by specifying the parameter name: res.params['parameter_name'].value\n",
    "#TODO: Set txy1 below to calculate the rate of diffusion.\n",
    "\n",
    "#Finally we calculate diffusion from our transit time measurement.\n",
    "print('D ',((1.0/np.sqrt(2.0*np.log(2.0)))*float(file_dict['FWHMs'][0])/1000.0)**2/(4*txy1)*1000.0)\n",
    "#You should get around 0.62 um2/s\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Fitting\n",
    "Gaussian fitting is one of the most common procedures to perform in N-Dimensional signal analysis (i.e. 1-D sequences, 2-D images and 3-D volumes).Furthermore general education relating to Gaussian functions, their properties and their varying anisotropies will be added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here is a handy function which returns a 2-D Gaussian.\n",
    "def Gaussian_2D(xx,yy,x_mu, y_mu, sig_x, sig_y, A):\n",
    "    f = A*np.exp(-(((xx-x_mu)**2/(2*sig_x**2))+((yy-y_mu)**2/(2*sig_y**2))))\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In this cell, we generate and visualise our own Gaussian distribution: \n",
    "\n",
    "#Parameters of 2-D Gaussian distribution.\n",
    "x_mu = 50 #Mean of Gaussian in x.\n",
    "y_mu = 25 #Mean of Gaussian in y.\n",
    "sig_x = 10 #Sigma of Gaussian in x (i.e. width).\n",
    "sig_y = 10 #Sigma of Gaussian in y (i.e height).\n",
    "A = 5 #Amplitude of Gaussian (i.e what the peak intensity value is).\n",
    "\n",
    "#Generate grid.\n",
    "xrange = np.arange(-100,101,1)\n",
    "yrange =  np.arange(-100,101,1)\n",
    "xx,yy = np.meshgrid(xrange,yrange)\n",
    "\n",
    "#Evaluate function over grid.\n",
    "f = Gaussian_2D(xx,yy,x_mu, y_mu, sig_x, sig_y, A)\n",
    "\n",
    "plt.imshow(f,extent=[-100,100,100,-100])\n",
    "plt.xlabel('x coordinates');\n",
    "plt.ylabel('y coordinates');\n",
    "#TODO:\n",
    "#Play with the parameters. Try to get the Gaussian in the top-left corner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Estimate parameters:\n",
    "#Parameters of 2-D Gaussian distribution.\n",
    "x_mu = 5 #Mean of Gaussian in x.\n",
    "y_mu = 25 #Mean of Gaussian in y.\n",
    "sig_x = 10 #Sigma of Gaussian in x (i.e. width).\n",
    "sig_y = 10 #Sigma of Gaussian in y (i.e height).\n",
    "A = 5 #Amplitude of Gaussian (i.e what the peak intensity value is).\n",
    "\n",
    "f = Gaussian_2D(xx,yy,x_mu, y_mu, sig_x, sig_y, A)\n",
    "\n",
    "#Below are the estimations for our initial parameters. These equations look worse than they really are.\n",
    "#They represent the weighted mean and weighted sample standard deviation.\n",
    "#https://en.wikipedia.org/wiki/Weighted_arithmetic_mean\n",
    "#Essentially they provide us rough parameters of our Gaussian.\n",
    "x_plot = np.sum(f,0) #we integrate with y with respect to x.\n",
    "y_plot = np.sum(f,1) #we integrate with x with respect to y.\n",
    "x_mu_est = np.sum(xrange*x_plot)/np.sum(x_plot) #Our Gaussian mean in the x-dimension.\n",
    "y_mu_est = np.sum(yrange*y_plot)/np.sum(y_plot) #Our Gaussian mean in the y-dimension.\n",
    "sig_x_est =np.sqrt(np.sum(x_plot*(xrange-x_mu_est)**2)/((xrange.shape[0]-1)*(np.sum(x_plot)/(xrange.shape[0])))) \n",
    "sig_y_est =np.sqrt(np.sum(y_plot*(yrange-y_mu_est)**2)/((yrange.shape[0]-1)*(np.sum(y_plot)/(yrange.shape[0]))))\n",
    "A_est = np.max(f)#An easy one.\n",
    "\n",
    "print('Should closely resemble our inputs.')\n",
    "print('x_mu estimation:',x_mu_est,'y_mu estimation:',y_mu_est)\n",
    "print('Amplitude (A) estimation:',A_est)\n",
    "print('sig_x estimation:',sig_x_est)\n",
    "print('sig_y estimation:',sig_y_est)\n",
    "\n",
    "#Analytical methods like the above work, but are susceptible to noise or outliers.\n",
    "#They do however provide a good estimate from which to do a finer grain fitting.\n",
    "plt.imshow(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is where we try and fit our 2-D Gaussian.\n",
    "#First we initialise our parameters:\n",
    "param = Parameters()\n",
    "param.add('x_mu', value=x_mu_est, min=np.min(xx), max=np.max(xx), vary=True); \n",
    "param.add('y_mu', value=y_mu_est, min=np.min(yy), max=np.max(yy), vary=True); \n",
    "param.add('sig_x', value=sig_x_est, min=0, max=xrange.shape[0], vary=True); \n",
    "param.add('sig_y', value=sig_y_est, min=0, max=yrange.shape[0], vary=True); \n",
    "param.add('A', value=A_est, min=0, max=yrange.shape[0], vary=True);\n",
    "\n",
    "#Next we define our residuals equation:\n",
    "def gauss_residuals(param,xx,yy,f):\n",
    "    #We pass our param object which gets dynamically changed by the optimisation equation.\n",
    "    #We also have xx and yy and f. These don't change and so are not in the param object.\n",
    "    f0 = Gaussian_2D(xx,yy,param['x_mu'].value, param['y_mu'].value, param['sig_x'].value, param['sig_y'].value, param['A'].value)\n",
    "    #We return our residuals, as a array of values.\n",
    "    return f0-f\n",
    "#This is where the magic happens, the minimisation:\n",
    "res = minimize(gauss_residuals, param, args=(xx.flatten(),yy.flatten(),f.flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here is a nice summary of how our fit went. Now our parameter predictions(shown below) are very close to our value.\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian fitting with noise\n",
    "We have seen above that numerical parameter estimation is fine and that fitting is a tiny bit better.\n",
    "Now we add some Poisson noise and see how well our estimation and fitting goes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Estimate parameters:\n",
    "#Parameters of 2-D Gaussian distribution.\n",
    "x_mu = 5 #Mean of Gaussian in x.\n",
    "y_mu = 25 #Mean of Gaussian in y.\n",
    "sig_x = 20 #Sigma of Gaussian in x (i.e. width).\n",
    "sig_y = 20 #Sigma of Gaussian in y (i.e height).\n",
    "A = 5 #Amplitude of Gaussian (i.e what the peak intensity value is).\n",
    "\n",
    "f = Gaussian_2D(xx,yy,x_mu, y_mu, sig_x, sig_y, A)\n",
    "f +=np.random.poisson(0.1,size=f.shape)\n",
    "\n",
    "x_plot = np.sum(f,0) #we integrate with y with respect to x.\n",
    "y_plot = np.sum(f,1) #we integrate with x with respect to y.\n",
    "x_mu_est = np.sum(xrange*x_plot)/np.sum(x_plot) #Our Gaussian mean in the x-dimension.\n",
    "y_mu_est = np.sum(yrange*y_plot)/np.sum(y_plot) #Our Gaussian mean in the y-dimension.\n",
    "sig_x_est =np.sqrt(np.sum(x_plot*(xrange-x_mu_est)**2)/((xrange.shape[0]-1)*(np.sum(x_plot)/(xrange.shape[0])))) \n",
    "sig_y_est =np.sqrt(np.sum(y_plot*(yrange-y_mu_est)**2)/((yrange.shape[0]-1)*(np.sum(y_plot)/(yrange.shape[0]))))\n",
    "A_est = np.max(f)#An easy one.\n",
    "print('--------numerical estimates--------')\n",
    "print('Should closely resemble our inputs but due to noise will not.')\n",
    "print('x_mu estimation:',x_mu_est,'y_mu estimation:',y_mu_est)\n",
    "print('Amplitude (A) estimation:',A_est)\n",
    "print('sig_x estimation:',sig_x_est)\n",
    "print('sig_y estimation:',sig_y_est)\n",
    "\n",
    "#Numerical methods like the above work, but are susceptible to noise.\n",
    "#They do however provide a good estimate from which to do a finer grain fitting.\n",
    "plt.imshow(f)\n",
    "\n",
    "#This is where we try and fit our 2-D Gaussian.\n",
    "#We initialise our parameters.\n",
    "param = Parameters()\n",
    "param.add('x_mu', value=x_mu_est, min=np.min(xx), max=np.max(xx), vary=True); \n",
    "param.add('y_mu', value=y_mu_est, min=np.min(yy), max=np.max(yy), vary=True); \n",
    "param.add('sig_x', value=sig_x_est, min=0, max=xrange.shape[0], vary=True); \n",
    "param.add('sig_y', value=sig_y_est, min=0, max=yrange.shape[0], vary=True); \n",
    "param.add('A', value=A_est, min=0, max=2*np.max(f), vary=True);\n",
    "\n",
    "res = minimize(gauss_residuals, param, args=(xx.flatten(),yy.flatten(),f.flatten()))\n",
    "res.params['x_mu'].value\n",
    "print('\\n--------fitting outputs--------')\n",
    "print('Should closely resemble our inputs. Fitting is more robust.')\n",
    "print('x_mu fit value:',res.params['x_mu'].value,'y_mu fit value:',res.params['y_mu'].value,)\n",
    "print('Amplitude (A) fit value:',res.params['A'].value,)\n",
    "print('sig_x fit value:',res.params['sig_x'].value)\n",
    "print('sig_y fit value:',res.params['sig_y'].value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is to make the Gaussian fitting to patches easier. You will need this for the below TODO.\n",
    "\n",
    "def fit_Gaussian_to_img(img,sig_x_est,sig_y_est):\n",
    "    \"\"\"This function returns the parameters of a 2D Gaussian fit\n",
    "    input\n",
    "    -----------\n",
    "    img        -  Your 2-D input image.\n",
    "    sig_x_est  - The estimated sigma in x.\n",
    "    sig_y_est  -  The estimated sigma in y.\n",
    "    \n",
    "    outputs\n",
    "    -----------\n",
    "    x, y, sigma_x, sigma_y, amplitude - fitted parameters\n",
    "    \n",
    "    \"\"\"\n",
    "    xrange = np.arange(0,img.shape[1])\n",
    "    yrange =  np.arange(0,img.shape[0])\n",
    "    xx,yy = np.meshgrid(xrange,yrange)\n",
    "    param = Parameters()\n",
    "    param.add('x_mu', value=img.shape[1]//2, min=0, max=img.shape[1], vary=True); \n",
    "    param.add('y_mu', value=img.shape[0]//2, min=0, max=img.shape[0], vary=True); \n",
    "    param.add('sig_x', value=sig_x_est, min=0, max=20, vary=True); \n",
    "    param.add('sig_y', value=sig_y_est, min=0, max=20, vary=True); \n",
    "    param.add('A', value=np.max(img), min=0, max=2*np.max(img), vary=True);\n",
    "    \n",
    "    res = minimize(gauss_residuals, param, args=(xx.flatten(),yy.flatten(),img.flatten()))\n",
    "    \n",
    "    return res.params['x_mu'].value,res.params['y_mu'].value,res.params['sig_x'].value,res.params['sig_y'].value,res.params['A'].value\n",
    "\n",
    "patch = g_img[40:100,80:150]\n",
    "plt.imshow(patch)\n",
    "a = fit_Gaussian_to_img(patch,10.,10.)\n",
    "print(a)\n",
    "plt.plot(a[0],a[1],'o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here I have simulated a number of Gaussians.\n",
    "g_img = io.imread('images/2D_Gaus_sig4.png')\n",
    "plt.imshow(g_img)\n",
    "#Optional TODO: Using your code from the segmentation practical, find the Gaussians and estimate their parameters by fitting.\n",
    "#Hint: You will need to find the maxima locations. This can be done through segmentation or through find maxima.\n",
    "#Hint: With segmentation you can use the regionprops to return the centroid locations.\n",
    "#Hint: Once you have the approximate location of the Gaussians then you need to extract each region and fit the 2-D Gaussians to them. (Use the function above: fit_Gaussian_to_img).\n",
    "#Print the parameters below your code \"gauss\",num,\"x_coord:\",x_coord,\"y_coord\"\n",
    "\n",
    "#Simulate the original image to make sure you have found the parameters correctly. This will involve generating the Gaussians one-by-one and adding them to the same image.\n",
    "#Plot the original image and your simulated image side-by-side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optional TODO: \n",
    "#Add Noise to the image and see how the technique reduces in accuracy. Don't worry about quantifying this, just observe it for yourself.\n",
    "#Your might want to use function: np.random.poisson(0.3,size=f.shape)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
